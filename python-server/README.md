# üöÄ Servidor Python FastAPI - Customer Intelligence ML

Servidor FastAPI que ejecuta los modelos de Machine Learning entrenados para predicci√≥n de calidad de leads y churn.

## üìã Requisitos

- Python 3.9 o superior
- Modelos entrenados en `/ml/models/` (ejecuta primero `python ml/train_leads_and_churn.py`)

## üîß Instalaci√≥n

### 1. Crear entorno virtual (recomendado)

\`\`\`bash
cd python-server
python3 -m venv venv
source venv/bin/activate  # En Windows: venv\Scripts\activate
\`\`\`

### 2. Instalar dependencias

\`\`\`bash
pip install -r requirements.txt
\`\`\`

## üéØ Uso

### Iniciar el servidor

**Opci√≥n 1: Usando el script (recomendado)**

\`\`\`bash
chmod +x python-server/start-server.sh
./python-server/start-server.sh
\`\`\`

**Opci√≥n 2: Manualmente**

\`\`\`bash
cd python-server
python main.py
\`\`\`

El servidor estar√° disponible en:
- **API**: http://localhost:8000
- **Documentaci√≥n interactiva (Swagger)**: http://localhost:8000/docs
- **Redoc**: http://localhost:8000/redoc

### Health Check

\`\`\`bash
curl http://localhost:8000/health
\`\`\`

Respuesta esperada:
\`\`\`json
{
  "status": "healthy",
  "models": {
    "lead_quality": true,
    "churn": true
  },
  "message": "Todos los modelos cargados"
}
\`\`\`

## üì° Endpoints

### 1. POST `/predict/lead-quality`

Predice la calidad de un lead (caliente, tibio, fr√≠o).

**Request Body:**
\`\`\`json
{
  "name": "Juan P√©rez",
  "city": "Bogot√°",
  "channel": "WhatsApp Bot",
  "budget": 15000000,
  "urgency": 4,
  "service_type": "Social Ads"
}
\`\`\`

**Response:**
\`\`\`json
{
  "quality_label": "caliente",
  "quality_score": 0.85,
  "probabilities": {
    "fr√≠o": 0.05,
    "tibio": 0.10,
    "caliente": 0.85
  }
}
\`\`\`

### 2. POST `/predict/churn`

Predice la probabilidad de churn de un cliente.

**Request Body:**
\`\`\`json
{
  "client_id": "CLI-001",
  "engagement": "Medio",
  "satisfaccion": "Alto",
  "dias_ultima_compra": 45,
  "total_compras": 50000000,
  "promedio_compra": 10000000,
  "num_transacciones": 5,
  "std_compra": 2000000
}
\`\`\`

**Response:**
\`\`\`json
{
  "client_id": "CLI-001",
  "churn_probability": 0.35,
  "risk_level": "Medio"
}
\`\`\`

## üß™ Testing desde Next.js

1. Inicia el servidor Python (puerto 8000)
2. Inicia Next.js (puerto 3000)
3. Ve a http://localhost:3000/bot
4. Completa el formulario del bot
5. La predicci√≥n usar√° tu modelo ML real

## üêõ Troubleshooting

### Error: "Modelo no disponible"

\`\`\`bash
# Entrena los modelos primero
python ml/train_leads_and_churn.py
\`\`\`

Verifica que existan estos archivos:
- `ml/models/lead_quality_model.joblib`
- `ml/models/lead_quality_scaler.joblib`
- `ml/models/feature_config_leads.json`
- `ml/models/churn_model.joblib`
- `ml/models/churn_scaler.joblib`
- `ml/models/feature_config_churn.json`

### Error: "ModuleNotFoundError: No module named 'ml'"

Aseg√∫rate de ejecutar el servidor desde la ra√≠z del proyecto:
\`\`\`bash
# ‚úÖ Correcto
cd /ruta/al/proyecto
python python-server/main.py

# ‚ùå Incorrecto
cd python-server
python main.py
\`\`\`

### Puerto 8000 en uso

\`\`\`bash
# Mata el proceso que usa el puerto
lsof -ti:8000 | xargs kill -9

# O usa otro puerto
uvicorn main:app --port 8001
\`\`\`

## üìä Monitoreo

Ver logs en tiempo real:
\`\`\`bash
tail -f python-server.log
\`\`\`

## üîê Producci√≥n

Para desplegar en producci√≥n:

1. Usa un servidor ASGI robusto como Gunicorn:
\`\`\`bash
gunicorn main:app --workers 4 --worker-class uvicorn.workers.UvicornWorker --bind 0.0.0.0:8000
\`\`\`

2. Configura CORS apropiadamente en `main.py`
3. Usa variables de entorno para configuraci√≥n sensible
4. Implementa rate limiting y autenticaci√≥n

## üìö Arquitectura

\`\`\`
Cliente (Next.js)
    ‚Üì
API Routes (/api/predict/lead-quality/route.ts)
    ‚Üì HTTP POST
FastAPI Server (Python - Puerto 8000)
    ‚Üì
ML Utils (ml/utils.py)
    ‚Üì
Modelos Entrenados (ml/models/*.joblib)
\`\`\`

## ü§ù Integraci√≥n con Next.js

Los API routes de Next.js en `/app/api/predict/lead-quality/route.ts` ahora hacen fetch a este servidor Python en lugar de usar heur√≠sticas.

Flujo completo:
1. Usuario completa bot en `/bot`
2. Bot llama a `apiClient.post("/predict/lead-quality")`
3. Next.js API route (`/api/v1/predict/lead-quality/route.ts`) hace fetch a Python
4. FastAPI ejecuta modelo real
5. Respuesta regresa al usuario con predicci√≥n real

**¬°Sin mocks, sin simulaciones, solo modelos reales! üéØ**
